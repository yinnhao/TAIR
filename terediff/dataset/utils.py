import random
import math
import os
import json 
import numpy as np
from PIL import Image
import cv2
import torch
from .diffjpeg import DiffJPEG
from torch.nn import functional as F


# unicode conversion: char <-> int
# use chr() and ord()
# char_table = [chr(i) for i in range(32,127)]
# valid_voc = list(string.printable[:-6])
# invalid_voc=['â–¡', 'âˆ«', 'Ã¦', 'â¬', 'Î£', 'â– ', 'Ã…', 'á¸', 'Â£', 'Å„', 'âŒ€', 'Ã™', 'â”‚', 'â…¦', 'Ã‚', 'Ï‚', 'â…«', 'â´', 'ÑŠ', 'âˆ', 'Ã†', 'Î±', 'Ã‡', 'Ë£', 'ãƒ»', 'â¤¤', 'Ä', 'Ä±', 'â‰¡', 'â‹„', 'â„«', 'á´´', 'áµ—', 'È‚', 'Î´', 'ÃŒ', 'Î¡', 'âŸ·', 'Ã¯', 'Â«', 'È¯', 'Ç‘', 'â‡©', 'Î¶', 'âœ°', 'â¹', 'Ğ¼', 'á»˜', 'â˜', 'â‚„', 'Â²', 'Ï†', 'âŒ´', 'â‡¨', 'ÆŒ', 'Ïƒ', 'â…¨', 'âˆ', 'Å£', 'Å¯', 'â—', 'Â½', 'Â¾', 'á´¾', 'ï¿½', 'Ãª', 'â…¥', 'Ë¢', 'Â°', 'É®', 'â‡ª', 'áµˆ', 'Ä–', 'Ç', 'âŠ²', 'Â·', 'Ã»', 'Ë…', 'âŠ¤', 'â†°', 'Äª', 'È', 'Ã—', 'âŠ', 'â€Ÿ', 'âˆš', 'â€', 'Ã®', 'â†¹', 'â', 'â†‘', 'Ã¼', 'â‹', 'â„ƒ', 'Ã›', 'È„', 'â€º', 'âŸ¶', 'â—‹', 'â“‡', 'ÈŠ', 'âœ', 'á´º', 'Ã¥', 'â–º', 'Ë‚', 'Î¹', 'Ä', 'Åš', 'âˆ‡', 'â€¢', 'Â¥', 'â˜…', 'â‹…', 'â‚–', 'Å©', 'â¼', 'Ä°', 'âˆ“', 'âŠ‚', 'â¯', 'â‚…', 'á»’', 'Â»', 'Å½', 'Ã¬', 'â…¤', 'â€', 'Å‡', 'Ãº', 'â€‘', 'Ã„', 'âŠ£', 'Ë„', 'Ë™', 'Ã“', 'Â±', 'â•³', 'â¿', 'Å«', 'ÅŸ', 'Ğ»', 'á¹ ', 'á´µ', 'È', 'Ã±', 'Î»', 'âœ“', 'Ã¸', 'âœ', 'â‰¤', 'Ã•', 'â¯', 'â¬Œ', 'Ê³', 'Å ', 'â—‰', 'â¨', 'á¶œ', 'Åº', 'Ä¡', 'Ã¿', 'â—¦', 'á¸»', 'â®', 'á´¸', 'Ãš', 'â”€', 'â‡§', 'â¤¶', 'Ã°', 'Ã«', 'Î', 'È‘', 'â‡¦', 'â†»', 'Äƒ', 'Äš', 'Î©', 'Ã', 'â‚ƒ', 'Ğº', 'â… ', 'â–¬', 'â€”', 'âˆˆ', 'áº ', 'â˜', 'â¸', 'Å”', 'Ã¹', 'Ã¢', 'Ğ¿', 'á´­', 'Ã·', 'â†²', 'â€˜', 'È†', 'áµ€', 'Â¿', 'Èš', 'â–', 'Ä›', 'â±½', 'Î›', 'âˆ·', 'â–³', 'Ã§', 'Ç«', 'áº¦', 'â©', 'Ğ¸', 'Åª', 'Ã½', 'â€•', 'â‡µ', 'Ã', 'ê‹', 'â†“', 'Â©', 'Â³', 'Æ†', 'Ã¨', 'ğŸ ˆ', 'ÄŸ', 'â’¶', 'Ñ', 'Î¦', 'áº¤', 'áµ–', 'ï¸½', 'Ëš', 'Å“', 'âˆ¥', 'Î²', 'Ğ¹', 'â’¸', 'â¬', 'âˆ¨', 'â„®', 'Â¼', 'Ä‡', 'â£', 'Ãƒ', 'ğŸ¡¨', 'Ä„', 'Çµ', 'â„¢', 'áº¾', 'áµ', 'â—„', 'Åƒ', 'âœ±', 'Ã´', 'Â¢', 'â‚', 'â…¡', 'Â¹', 'Ï€', 'Âµ', 'Ä¹', 'â™', 'Ñ€', 'Ã', 'Îµ', 'âŸµ', 'âˆ†', 'Ñ‹', 'â§«', 'Ã£', 'Ä—', 'â°', 'â¬‰', 'âˆ’', 'â¬‹', 'â—¯', 'Ğ¾', 'Ã€', 'Ï', 'â˜°', 'Ï„', 'Å—', 'â¸¬', 'Ã–', 'Ã©', 'É™', 'Çª', 'Ä’', 'âµ', 'ğ”€', 'â“’', 'È', 'â€œ', 'ÄŒ', 'Ä', 'Ã', 'âˆ™', 'á¹£', '\u200b', 'âœš', 'Å', 'â€', 'Ã¶', 'á´¹', 'â–¢', 'Î½', 'âŒ£', 'ï¼š', 'ï¸¾', 'ï¹˜', 'Ğ°', 'âˆ–', 'âŒ„', 'Ğ²', 'ï¸¿', 'áµƒ', 'á»›', 'â†º', 'â–²', 'â–½', 'â€¦', 'Ã‹', 'âŒ«', 'â¤·', 'â‚¬', 'âŠ˜', 'Å', 'â‚‚', 'â¤º', 'âµ', 'È¦', 'âˆ§', 'Ï‰', 'å', 'â…£', 'â»', 'â†µ', 'Ä©', 'â…¢', 'Ä‚', 'â¬¸', 'Êƒ', 'È‡', 'â†', 'â…“', 'â®Œ', 'â‡¥', 'Î·', 'â¦', 'Ã”', 'â¬Š', 'â„‰', 'âŠ¥', 'Ã¡', 'Å‰', 'âŠš', 'â€“', 'Ä€', 'âˆ…', 'Ä†', 'âˆ', 'â¤¸', 'â¦', 'Ä“', 'Î¯', 'Ãµ', 'á´±', 'Ï…', 'ÃŸ', 'â—¡', 'Ãˆ', 'âˆ£', 'Î”', 'á´™', 'Ã²', 'âŠ¢', 'Îº', 'â˜“', 'á»€', 'Î˜', 'Ã¤', 'ï¹€', 'â˜†', 'Ã’', 'Ëƒ', 'Ã ', 'ÃŠ', 'Ê°', 'Ä', 'â€™', 'â†’', 'Â®', 'â—', 'âº', 'Å¢', 'Å»', 'Ì“', 'â–¼', 'á»‚', 'áµ’', 'Ã', 'Ğ±', 'â”', 'Ğ³', 'âˆ´', 'â…”', 'â¬ˆ', 'ÅŒ', 'âˆŠ', 'Î ', 'â…§', 'Ã‘', 'â', 'Ã‰', 'Å', 'Ã³', 'âˆ‰', 'Ã˜', 'Ãœ', 'â‹®', 'Äº', 'â‰£', 'âˆ¼', 'â†±', 'Ã­', 'â…©', 'Ä™', 'â‹¯', 'Ñ', 'â•', 'â¤¦', 'âŠ¼', 'È§', 'âˆ', 'â¤»', 'Î¾', 'Å¡', 'â–¾', 'Î³', 'Â¡', 'âŠ³', 'Ğ´', 'â·', 'Ğ¶', 'â§', 'á´°', 'â€§', 'âˆ˜', 'Å¾', 'È®', 'â…ª']
CTLABELS = [' ','!','"','#','$','%','&','\'','(',')','*','+',',','-','.','/','0','1','2','3','4','5','6','7','8','9',':',';','<','=','>','?','@','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','[','\\',']','^','_','`','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','{','|','}','~']


def decode(idxs):
    s = ''
    for idx in idxs:
        if idx < len(CTLABELS):
            s += CTLABELS[idx]
        else:
            return s
    return s


def encode(word):
    s = []
    max_word_len = 25
    for i in range(max_word_len):
        if i < len(word):
            char=word[i]
            idx = CTLABELS.index(char)
            s.append(idx)
        else:
            s.append(96)
    return s


def load_file_list(file_list_path: str, data_args=None):

    mode = data_args['mode']
    datasets = data_args['datasets']
    ann_path = data_args['ann_path']
    model_H, model_W = data_args['model_img_size']

    files = []
    for dataset in datasets:

        if dataset == 'sam_cleaned_100k':
            
            # load json 
            json_path = ann_path 
            with open(json_path, 'r') as f:
                json_data = json.load(f)
                json_data = sorted(json_data.items())
            

            # split train and val ratio 10:1
            split_index = int(len(json_data) * 10 / 11)
            if mode == 'TRAIN':
                json_data = dict(json_data[:split_index])
            elif mode == 'VAL':
                json_data = dict(json_data[split_index:])


            # image path 
            imgs_path = f'{file_list_path}/images'
            imgs = sorted(os.listdir(imgs_path))


            for img in imgs:
                gt_path = f'{imgs_path}/{img}'

                img_id = img.split('.')[0]
                if img_id in json_data.keys():
                    img_ann = json_data[img_id]['0']['text_instances']
                else:
                    continue


                boxes=[]
                texts=[]
                text_encs=[]
                polys=[]

                for ann in img_ann:

                    # process text 
                    text = ann['text']
                    count=0
                    for char in text:
                        # only allow OCR english vocab: range(32,127)
                        if 32 <= ord(char) and ord(char) < 127:
                            count+=1
                            # print(char, ord(char))
                    if count == len(text) and count < 26:
                        texts.append(text)
                        text_encs.append(encode(text))
                        assert text == decode(encode(text)), 'check text encoding !'
                    else:
                        continue


                    # process box
                    box_xyxy = ann['bbox']
                    x1,y1,x2,y2 = box_xyxy
                    box_xywh = [ x1, y1, x2-x1, y2-y1 ]
                    box_xyxy_scaled = list(map(lambda x: x/model_H, box_xyxy))  # scale box coord to [0,1]
                    x1,y1,x2,y2 = box_xyxy_scaled 
                    box_cxcywh = [(x1+x2)/2, (y1+y2)/2, x2-x1, y2-y1]   # xyxy -> cxcywh
                    # box format
                    processed_box = box_cxcywh
                    processed_box = list(map(lambda x: round(x,4), processed_box))
                    boxes.append(processed_box)


                    # process polygons
                    poly = np.array(ann['polygon']).astype(np.int32)    # 16 2
                    # scale poly
                    poly_scaled = poly / np.array([model_W, model_H])
                    polys.append(poly_scaled)


                    # # VISUALIZE FOR DEBUGGING
                    # img0 = cv2.imread(gt_path)  # 512 512 3
                    # x,y,w,h = box_xywh
                    # cv2.rectangle(img0_box, (x,y), (x+w, y+h), (0,255,0), 2)
                    # cv2.polylines(img0_poly, [poly], True, (0,255,0), 2)
                    # cv2.putText(img0_box, text, (poly[0][0], poly[0][1]-5), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1)
                    # cv2.putText(img0_poly, text, (poly[0][0], poly[0][1]-5), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1)
                    # cv2.imwrite('./img0_box.jpg', img0_box)
                    # cv2.imwrite('./img0_poly.jpg', img0_poly)

                assert len(boxes) == len(texts) == len(text_encs) == len(polys), f" Check loader!"

                # if the filetered image has no bbox and texts, skip it
                if len(boxes) == 0 or len(polys) == 0:
                    continue
            
                caption = [f'"{txt}"' for txt in texts]
                prompt = f"A realistic scene where the texts {', '.join(caption) } appear clearly on signs, boards, buildings, or other objects."

                files.append({"image_path": gt_path, 
                              "prompt": prompt, 
                              "text": texts, 
                              "bbox": boxes,
                              'poly': polys,
                              'text_enc': text_encs, 
                              "img_name": img_id})     
    

    if mode=='VAL':
        files = random.sample(files, 6)

    return files


# https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/image_datasets.py
def center_crop_arr(pil_image, image_size):
    # We are not on a new enough PIL to support the `reducing_gap`
    # argument, which uses BOX downsampling at powers of two first.
    # Thus, we do it by hand to improve downsample quality.
    while min(*pil_image.size) >= 2 * image_size:
        pil_image = pil_image.resize(
            tuple(x // 2 for x in pil_image.size), resample=Image.BOX
        )

    scale = image_size / min(*pil_image.size)
    pil_image = pil_image.resize(
        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC
    )

    arr = np.array(pil_image)
    crop_y = (arr.shape[0] - image_size) // 2
    crop_x = (arr.shape[1] - image_size) // 2
    return arr[crop_y : crop_y + image_size, crop_x : crop_x + image_size]


# https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/image_datasets.py
def random_crop_arr(pil_image, image_size, min_crop_frac=0.8, max_crop_frac=1.0):
    min_smaller_dim_size = math.ceil(image_size / max_crop_frac)
    max_smaller_dim_size = math.ceil(image_size / min_crop_frac)
    smaller_dim_size = random.randrange(min_smaller_dim_size, max_smaller_dim_size + 1)

    # We are not on a new enough PIL to support the `reducing_gap`
    # argument, which uses BOX downsampling at powers of two first.
    # Thus, we do it by hand to improve downsample quality.
    while min(*pil_image.size) >= 2 * smaller_dim_size:
        pil_image = pil_image.resize(
            tuple(x // 2 for x in pil_image.size), resample=Image.BOX
        )

    scale = smaller_dim_size / min(*pil_image.size)
    pil_image = pil_image.resize(
        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC
    )

    arr = np.array(pil_image)
    crop_y = random.randrange(arr.shape[0] - image_size + 1)
    crop_x = random.randrange(arr.shape[1] - image_size + 1)
    return arr[crop_y : crop_y + image_size, crop_x : crop_x + image_size]


# https://github.com/XPixelGroup/BasicSR/blob/master/basicsr/data/transforms.py
def augment(imgs, hflip=True, rotation=True, flows=None, return_status=False):
    """Augment: horizontal flips OR rotate (0, 90, 180, 270 degrees).

    We use vertical flip and transpose for rotation implementation.
    All the images in the list use the same augmentation.

    Args:
        imgs (list[ndarray] | ndarray): Images to be augmented. If the input
            is an ndarray, it will be transformed to a list.
        hflip (bool): Horizontal flip. Default: True.
        rotation (bool): Ratotation. Default: True.
        flows (list[ndarray]: Flows to be augmented. If the input is an
            ndarray, it will be transformed to a list.
            Dimension is (h, w, 2). Default: None.
        return_status (bool): Return the status of flip and rotation.
            Default: False.

    Returns:
        list[ndarray] | ndarray: Augmented images and flows. If returned
            results only have one element, just return ndarray.

    """
    hflip = hflip and random.random() < 0.5
    vflip = rotation and random.random() < 0.5
    rot90 = rotation and random.random() < 0.5

    def _augment(img):
        if hflip:  # horizontal
            cv2.flip(img, 1, img)
        if vflip:  # vertical
            cv2.flip(img, 0, img)
        if rot90:
            img = img.transpose(1, 0, 2)
        return img

    def _augment_flow(flow):
        if hflip:  # horizontal
            cv2.flip(flow, 1, flow)
            flow[:, :, 0] *= -1
        if vflip:  # vertical
            cv2.flip(flow, 0, flow)
            flow[:, :, 1] *= -1
        if rot90:
            flow = flow.transpose(1, 0, 2)
            flow = flow[:, :, [1, 0]]
        return flow

    if not isinstance(imgs, list):
        imgs = [imgs]
    imgs = [_augment(img) for img in imgs]
    if len(imgs) == 1:
        imgs = imgs[0]

    if flows is not None:
        if not isinstance(flows, list):
            flows = [flows]
        flows = [_augment_flow(flow) for flow in flows]
        if len(flows) == 1:
            flows = flows[0]
        return imgs, flows
    else:
        if return_status:
            return imgs, (hflip, vflip, rot90)
        else:
            return imgs


# https://github.com/XPixelGroup/BasicSR/blob/master/basicsr/utils/img_process_util.py
def filter2D(img, kernel):
    """PyTorch version of cv2.filter2D

    Args:
        img (Tensor): (b, c, h, w)
        kernel (Tensor): (b, k, k)
    """
    k = kernel.size(-1)
    b, c, h, w = img.size()
    if k % 2 == 1:
        img = F.pad(img, (k // 2, k // 2, k // 2, k // 2), mode="reflect")
    else:
        raise ValueError("Wrong kernel size")

    ph, pw = img.size()[-2:]

    if kernel.size(0) == 1:
        # apply the same kernel to all batch images
        img = img.view(b * c, 1, ph, pw)
        kernel = kernel.view(1, 1, k, k)
        return F.conv2d(img, kernel, padding=0).view(b, c, h, w)
    else:
        # img: torch.Tensor
        img = img.view(1, b * c, ph, pw)
        kernel = kernel.view(b, 1, k, k).repeat(1, c, 1, 1).view(b * c, 1, k, k)
        return F.conv2d(img, kernel, groups=b * c).view(b, c, h, w)


class USMSharp(torch.nn.Module):

    def __init__(self, radius=50, sigma=0):
        super(USMSharp, self).__init__()
        if radius % 2 == 0:
            radius += 1
        self.radius = radius
        kernel = cv2.getGaussianKernel(radius, sigma)
        kernel = torch.FloatTensor(np.dot(kernel, kernel.transpose())).unsqueeze_(0)
        self.register_buffer("kernel", kernel)

    def forward(self, img, weight=0.5, threshold=10):
        blur = filter2D(img, self.kernel)
        residual = img - blur

        mask = torch.abs(residual) * 255 > threshold
        mask = mask.float()
        soft_mask = filter2D(mask, self.kernel)
        sharp = img + weight * residual
        sharp = torch.clip(sharp, 0, 1)
        return soft_mask * sharp + (1 - soft_mask) * img
